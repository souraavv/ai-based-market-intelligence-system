import os
import sys
import random
import pickle
import joblib
import logging
import warnings
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib as mp
import matplotlib.pyplot as plt

from tcn import TCN
from datetime import datetime
from datetime import timedelta  
from keras.models import load_model
from tensorflow.keras import callbacks
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import MaxAbsScaler
from tensorflow.keras.models import Sequential 

from fillMissingData import interpolateFile
# from pushFirebase import push_recommendation
from prospectTheory import generateRecommendation, mergePredictionsOfModel

warnings.filterwarnings('ignore')
tf.get_logger().setLevel(logging.ERROR)

# --- Setup the stage -----
stage = "production"

# --- Setup the parameter
freq = 15   # This is fixed now to 15.  
storeDaysPrediction = [[] for i in range (0, 30)] # A non-persistent storage for the prediction generated by the model for ith day.

mandiA = 'RAJASTHAN_KOTA_Price.csv'
mandiB = 'MADHYA PRADESH_MAHIDPUR_Price.csv'
lagDays = 5

# ----- Setup all paths ---- .
parDir = '/home/baadalvm/Playground_Ronak_Sourav/Temp'
fileModelPath = f"{parDir}/Data/ModelData/Saved_Model"
predictedDataPath = f"{parDir}/Data/ModelData/Predicted_Data"
persistDataPath = f"{parDir}/Data/ModelData/Persist_Data"
fileYScalerPath = f"{parDir}/Data/ModelData/Saved_Scaler"
fileX0ScalerPath = f"{parDir}/Data/ModelData/Saved_Scaler"
fileX1ScalerPath = f"{parDir}/Data/ModelData/Saved_Scaler"
inputFilePath = f"{parDir}/Data/PlottingData/SOYABEAN/ARIMA_Interpolated_Data"

'''
---------------------------------------------------------------------------------------------
    Input     : Dataframe, inputSize, outputSize.
    Procedure : Split the data into train of length x - 30, and test of length 30(we are not using test data).
    Output    : trainData, testData : These are the split of original dataframe.
    Note      : PRICE_y corresponds to the prices of leading mandi and PRICE_x corresonds
                to the mandi which follow mandi B i.e A is lagging mandi
---------------------------------------------------------------------------------------------
'''
def getTrainTestSplit(df, inputSize, outputSize):
    df = df[['PRICE_x', 'PRICE_y']].values                    
    trainData, testData = np.array(df[:-(outputSize), :]), np.array(df[-(outputSize):, :]) 
    return trainData, testData


'''
---------------------------------------------------------------------------------------------
    Input     : Dataframe, inputSize, outputSize.
    Procedure : for each i belongs to (0, x - k) we will make [i, i + 60) as input to model (curX)
                and [i + 60, i + 90) as the actual output i.e curY.

                Since we are predicting prices for the lagging mandi, we will take that in the output.

                where x is length of trainData.
                k = 90 since inputSize = 60, outputSize = 30

    Output    : xTrain, yTrain : where xTrain is input the model and yTrain is the yHat(actual output)
    Note      : PRICE_y corresponds to the prices of leading mandi and PRICE_x corresonds
                to the mandi which follow mandi B i.e A is lagging mandi
      
---------------------------------------------------------------------------------------------
'''
def prepareTrainData(dataTrain, inputSize, outputSize):
    total = inputSize + outputSize
    xTrain, yTrain = [], []
    for i in range(0, len(dataTrain)-total):
        x = dataTrain[i:i+inputSize]
        y = dataTrain[i+inputSize:i+total, 0]
        xTrain.append(x)
        yTrain.append(y)
    xTrain = np.array(xTrain)
    yTrain = np.array(yTrain)
    return xTrain, yTrain

'''
--------------------------------------------------------
FineTuning Data prepare :

    Input:
        1) targetDf  : A dataframe which contains Price_x as Adilabad, and Price_y as its leading mandi
                         price(in our case Mahidpur)
        2) inputSize : 60, outputSize : 30

    Procedure:
        1) Only few chunks of data is available in the period 2006 Jan to 2021 Dec from Agmarknet
        2) We manually identify those chunks and that data is available in the file names present in files list.
        3) Since we are using multivariate, we need both prices of Adilabad and Mahidpur
        4) So for the selected chunks, we merge the data frame based on Dates. So for each chunk we have
           Price_X(Adilabad) and Price_Y(Mahidpur). We also shift mahidpur time series, where lag days relation
           of (Kota, Mahidpur) is used since we don't have enough data in Adilabad to compute the correlation and 
           the lag day corresponding to that.
        4) We are also obtaining data from CCD (through ODK forms). The data start from 19-Jan-2022
           and assumption is that this will be available for each day.
        5) We used both Agmarknet and ODK to form the finetune data
        
    Output:
        1) xTrainTL(x Train Transfer Learning) contains input for finetuning already trained model
           on (Kota, Mahidpur)
        2) yTrainTL contains outputs.
--------------------------------------------------------
'''

def prepareFineTuningData(idx, targetDf, inputSize, outputSize):
    # --- Files contians data only for the data where data is available ------ .
    files = [f'{parDir}/Adilabad/2014AR_TELANGANA_ADILABAD_AGMARKNET_Price.csv', 
            f'{parDir}/Adilabad/2015AR_TELANGANA_ADILABAD_AGMARKNET_Price.csv',
            f'{parDir}/Adilabad/2016AR_TELANGANA_ADILABAD_AGMARKNET_Price.csv']
    
    surrogateName = mandiB if idx == 0 else f'{idx}_{mandiB}'
    sourceLeadingMandiDF = pd.read_csv(f'{inputFilePath}/{surrogateName}')
    targetLaggingMandiDF = pd.read_csv(f"{inputFilePath}/TELANGANA_ADILABAD_Price.csv")
    # --- Shifting the leading mandi time series by lagDays ---
    lagDays = 3 # apx.
    sourceLeadingMandiDF['PRICE'] = sourceLeadingMandiDF['PRICE'].shift(periods = lagDays).ffill()
    xTrainTL, yTrainTL = [], []
    for file in files:
        df = pd.read_csv(file)
        merged_df = pd.merge(df, sourceLeadingMandiDF, how='inner', on =['DATE']) # Price_x should be of Adilabad and Price_y should be Bhawani
        trainData = merged_df[['PRICE_x', 'PRICE_y']].to_numpy()
        xt, yt = prepareTrainData(trainData, inputSize, outputSize)
        xTrainTL.extend(xt)
        yTrainTL.extend(yt)

    merged_df = pd.merge(targetLaggingMandiDF, sourceLeadingMandiDF, how='inner', on =['DATE']) # Price_x should be of Adilabad and Price_y should be Bhawani
    merged_df = merged_df[merged_df['DATE'] >= '2022-01-15'] 
    merged_df.reset_index(drop=True, inplace=True)
    targetTrainData, _ = getTrainTestSplit(merged_df, inputSize, outputSize)
    xt, yt = prepareTrainData(targetTrainData, inputSize, outputSize)
    xTrainTL.extend(xt)
    yTrainTL.extend(yt)


    # Append the data from 2022-01-19 onwards. targetDf already contains format : ['Price_x', 'Price_y']
    targetDf = targetDf[targetDf['DATE'] >= '2022-01-15'] 
    # targetDf already contains (Adilabad_Price, MahdipurPrice) where mahidpur is shifter by 5 days.
    targetTrainData, _ = getTrainTestSplit(targetDf, inputSize, outputSize)
    xt, yt = prepareTrainData(targetTrainData, inputSize, outputSize)
    xTrainTL.extend(xt)
    yTrainTL.extend(yt)

    xTrainTL = np.array(xTrainTL)
    yTrainTL = np.array(yTrainTL)
    return xTrainTL, yTrainTL


'''
---------------------------------------------------------------------------------------------
    Input     : xTrain(PRICE_x, PRICE_y), yTrain (PRICE_x) , xTest(PRICE_x, PRICE_y), yTest(PRICE_x)

    Procedure : Make Scaler object, which are using max and doing abs to get normalized data.
                            --- Input Data to Model (both xTrain and xTest)
                > Fit and transform different Scaler for PRICE_x corresponds to mandi_A i.e x0Scaler
                > Fit and transform different Scaler for PRICE_y corresponds to mandi_B i.e x1Scaler
                            --- Output Data from Model (both yTrain and yTest)
                > Fit and transform different Scaler for output data i.e PRICE_x using yScaler

    Output    : Transformed data and also return these scaler objects so that later when inverseTranform on data is applied we
                can use this. 
            
---------------------------------------------------------------------------------------------
'''

def transformData(xTrain, yTrain):
    x0Scaler, x1Scaler = MaxAbsScaler(), MaxAbsScaler()
    yScaler = MaxAbsScaler()
    xTrain[:, :,0], xTrain[:, :,1] = x0Scaler.fit_transform(xTrain[:, :,0]), x1Scaler.fit_transform(xTrain[:, :,1])
    yTrain = yScaler.fit_transform(yTrain)
    return xTrain, yTrain, x0Scaler, x1Scaler, yScaler

def scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

'''
---------------------------------------------------------------------------------------------
    Input     : xTrain(PRICE_x, PRICE_y), yTrain (PRICE_x) , xTest(PRICE_x, PRICE_y), yTest(PRICE_x)
                inputSize = 60, outputSize = 30, batchSize, learningRate(hyperparameter)

    Procedure : 
                > Define the model architecture for TCN 
                > Used Adam optimizer (used adaptive learning rate)
                > early Stopping criteria defined
                > Fit the model for the given xTrain and yTrain.

    Output    : Return the trained model.

---------------------------------------------------------------------------------------------
'''

def trainModel(xTrain, yTrain, inputSize, outputSize, learningRate, batchSize):
    # Define the architecture of the model
    model = Sequential([
                TCN(input_shape = (xTrain.shape[1], xTrain.shape[2])),
                Dense(outputSize)
    ])
    # Adam optmimzer
    optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)
    model.compile(loss = 'mse', optimizer = optimizer)
    earlyStopping = callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min', min_delta = 0.0001, restore_best_weights = True)
    lrScheduler = callbacks.LearningRateScheduler(scheduler)
    model.fit(xTrain, yTrain, epochs = 100, shuffle = False, validation_split = 0.2, callbacks = [earlyStopping, lrScheduler], verbose = 0, batch_size = batchSize)
    return model

def makeData3D(yTrain):
    yTrain = np.expand_dims(yTrain, axis = 2)
    return yTrain

'''
To generate Error model dataset.
    This function is used to prepared the data set for the error models
    It take input cnt, which represents how many number of error dataset we need
    Logic:
        It first find length of all the continuous available days, and similar to the continuos missing days.
        Then It shuffle these list randomly and prepare files in the Nans_Data which need the interpolation
        This logic also interpolate these files.
'''
def generateErrorModelDataset(cnt):
    print (f'\t\t----- Generating Error Model DataSet ...')
    efiles = []
    for mfile in [mandiA, mandiB]:
        mandi_df = pd.read_csv(f'{parDir}/Data/PlottingData/SOYABEAN/Nans_Data/{mfile}')
        mandi_df['DATE'] = pd.to_datetime(mandi_df['DATE'], format='%Y-%m-%d')
        missing_days, available_days = [], []
        missing_cnt, available_cnt = 0, 0
        for index, row in mandi_df.iterrows():
            # print(type(row.at['PRICE']))
            if pd.isnull(row['PRICE']):
                missing_cnt = missing_cnt + 1
                if available_cnt:
                    available_days.append(available_cnt)
                    available_cnt = 0
            else:
                available_cnt = available_cnt + 1
                if missing_cnt:
                    missing_days.append(missing_cnt)
                    missing_cnt = 0
        if missing_cnt:
            missing_days.append(missing_cnt)
        if available_cnt:
            available_days.append(available_cnt)
        
        for i in range(0, cnt):
            print(f'Generating dataset For the Model :{i+1}')
            random.shuffle(available_days)
            random.shuffle(missing_days)
            mss, mse = 0, len(missing_days)
            avs, ave = 0, len(available_days)
            interpolated_df = pd.read_csv(f'{parDir}/Data/PlottingData/SOYABEAN/ARIMA_Interpolated_Data/{mfile}')
            interpolated_df['DATE'] = pd.to_datetime(interpolated_df['DATE'], format='%Y-%m-%d')
            # interpolated_df = interpolated_df.loc[(interpolated_df['DATE'] >= '2006-01-01') & (interpolated_df['DATE'] <= '2021-12-31')]
            ci = 0
            while (mss < mse) and (avs < ave):
                interpolated_df.iloc[ci:(ci+missing_days[mss]-1), 1] = np.nan
                ci = ci + missing_days[mss] + available_days[avs]
                mss = mss + 1
                avs = avs + 1
            if mss < mse:
                interpolated_df.iloc[ci:(ci+missing_days[mss]-1), 1] = np.nan
            efiles.append(f'{i+1}_{mfile}')
            interpolated_df.to_csv(f'{parDir}/Data/PlottingData/SOYABEAN/Nans_Data_Temp/{i+1}_{mfile}', index=False)
    # call for the interpolation on the error files
    interpolateFile(efiles)

def updatePredictionFiles(errorModels):
    '''
    For all errorModels
    Day 0-> Day 29,
    Day 1-> Delete
    Day 2 to Day 29 -> Day 1 to Day 28
    '''
    flag = True
    prefFile = f'{predictedDataPath}/Error_Model'
    for errModelIdx in range(errorModels):
        # delete day 1.
        if os.path.exists(f'{prefFile}_{errModelIdx}_Day_1.csv'):
            # print ("Deleting Day 1 file...\n")
            os.remove(f'{prefFile}_{errModelIdx}_Day_1.csv') # delete only.
        else:
            flag = False
        # For day 2 to day 29
        for day in range(2, 30):
            if os.path.exists(f'{prefFile}_{errModelIdx}_Day_{day}.csv'):
                # print (f"renaming day {day} file to {day - 1} file\n")
                os.rename(f'{prefFile}_{errModelIdx}_Day_{day}.csv', f'{prefFile}_{errModelIdx}_Day_{day - 1}.csv') # rename
                # os.remove(f'{prefFile}_{errModelIdx}_Day_{day}.csv') #delete
            else:
                flag = False
        # For day 0.
        if os.path.exists(f'{prefFile}_{errModelIdx}_Day_0.csv'):
            # print ("created Day 29 file from Day 0 file\n")
            os.rename(f'{prefFile}_{errModelIdx}_Day_0.csv', f'{prefFile}_{errModelIdx}_Day_29.csv') # rename
            # os.remove(f'{prefFile}_{errModelIdx}_Day_0.csv') # delete
        else:
            flag = False
    return flag

'''
---------------------------------------------------------------------------------------------
    Input     :  dataFrame containing data : df, save_file_idx, curDay, nextTrainDay, inputSize, outputSize, learningRate, batchSize
    Procedure :  It will recieved a dataframe as input say df.
                        > First split that df in to trainData and testData # To do use test Data so that we can have both predicted as well as Actual
                        > Split : If len(df) = n, then n - 60 => Train and 60 => Test # If we want to use it for have yTest then increment start + 30 and then take out 90 from the last (60 input of test) and (30 output of test)
                        > From trainData take out xTrain, yTrain
                        > Transform the data and obtained the scaler which are used to scaled (used MaxAbsScaler)
                        > Make the Data 3D
                        > Train the model on given xTrain and its corresponding ground truth yTrain.
                        > Test the model : Used the trained model to predict (yPredict) on the yTrain
                        > Convert yPredict and yTrain into original form by inverseTranform

                        > It will store the output of the predictions in the two dimensional list at the index : save_file_idx, this data is later
                          transfer to the "Day_{i}.csv" files. Didn't saved directly to avoid lot of I/O into disk. 

    Output : 
                List containing 30 predicted values.                       
--------------------------------------------------------------------------------------------- 
'''
def predictEveryDay(idx, sourceDf, targetDf, save_file_idx, curDay, nextTrainDay, inputSize, outputSize, learningRate, batchSize):
    if curDay == nextTrainDay:
        print (f' \t\tFor Model : {idx}, Day : {save_file_idx + 1} \nThis is a training Day, Prediction will happen after training ----- ')
        # sourceDf and targetDf are the dataframes of Rajasthan Kota and Madhya Pradesh Mahidpur mandis.
        sourceTrainData, _ = getTrainTestSplit(sourceDf, inputSize, outputSize)
        xTrain, yTrain = prepareTrainData(sourceTrainData, inputSize, outputSize)
        xTrainTL, yTrainTL = prepareFineTuningData(idx, targetDf, inputSize, outputSize) 
     
        # xTrain = np.concatenate([xTrain, xTrainTL], axis=0)
        # yTrain = np.concatenate([yTrain, yTrainTL], axis=0)

        xTrain, yTrain, x0Scaler, x1Scaler, yScaler = transformData(xTrain, yTrain) # Save these scaler for next k days.
        yTrain = makeData3D(yTrain)
        model = trainModel(xTrain, yTrain, inputSize, outputSize, learningRate, batchSize) 
        # Save the model For futher use in next k days(including today)
        model.save(f'{fileModelPath}/Model_{idx}/', include_optimizer=False)        
        # Save the Scalers 
        joblib.dump(yScaler, f'{fileYScalerPath}/yScaler_{idx}.save') 
        joblib.dump(x0Scaler, f'{fileX0ScalerPath}/x0Scaler_{idx}.save') 
        joblib.dump(x1Scaler, f'{fileX1ScalerPath}/x1Scaler_{idx}.save') 

    print (f' \t\t---- Starting prediction for Model {idx} and Day : {save_file_idx + 1}---- ')
    # load the model
    model = load_model(f'{fileModelPath}/Model_{idx}/')
    # load the Scalers
    x0Scaler = joblib.load(f'{fileYScalerPath}/x0Scaler_{idx}.save')
    x1Scaler = joblib.load(f'{fileYScalerPath}/x1Scaler_{idx}.save')
    yScaler = joblib.load(f'{fileYScalerPath}/yScaler_{idx}.save')
    
    xTest = sourceDf[['PRICE_x', 'PRICE_y']].values
    xTest = xTest[-(inputSize):, :]
    xTest = np.array(xTest)
    # Phase of preparting test data.
    xCur = []
    xCur.append(xTest)
    xCur = np.array(xCur)
    xCur[:, :, 0] = x0Scaler.transform(xCur[:, :, 0])
    xCur[:, :, 1] = x1Scaler.transform(xCur[:, :, 1])
    # Used the model to Predict.
    yPredicted = model.predict(xCur)
    yPredicted = yScaler.inverse_transform(yPredicted) # 30 values for day i in cycle of freq days
    yPredicted = yPredicted[0, :]
    storeDaysPrediction[save_file_idx] += yPredicted.tolist() 
    print (f'Appending predictions to the {save_file_idx + 1} storeDaysPrediction and cur len = {len(storeDaysPrediction[save_file_idx])}')
    return yPredicted.tolist()

'''
---------------------------------------------------------------------------------------------
    Input : 
                > Dataframe from the both mandis 
    Procedure:
                > Shift leadingMandi by k, where k is the lag day at max correlation b/w mandis
                > Shift only happen for prices, not for dates.
                > merge both
                > Since first k values in leadingMandi is NaN thus take out first k value from the merged df
    Output:
                > Merged Data Frame df, where columns ['DATE', 'PRICE_x', 'PRICE_y']
---------------------------------------------------------------------------------------------
'''
def mergeMandisData(leadingMandiDF, laggingMandiDF, lagDays = -100):
    # overwrite the value of k with lagDays, if provided. 
    # because leading mandi and lag days are fixed when we are forecating for Adilabad.
    k = lagDays
    if k == -100:
        x = dict()
        for i in range(-10, 10):
            x[i] = laggingMandiDF['PRICE'].corr(leadingMandiDF['PRICE'].shift(i))
        k = max(x, key = x.get)
        
    leadingMandiDF['PRICE'] = leadingMandiDF['PRICE'].shift(periods = k).ffill()
    df = pd.merge(laggingMandiDF, leadingMandiDF, on = 'DATE', how = 'inner')
    df = df.iloc[k:].reset_index(drop = True)
    return df

'''
---------------------------------------------------------------------------------------------
        Input : stage, save_file_idx.
        Procedure: 
                > If production then we need to save just for the start_file_idx day only.
                > This logic simply pick the data from storeDaysPrediction[save_file_idx], which is the current day index range = [0, 29] and make it
                  persist to a csv file. So that later it can be used by prospect theory.
        Output: 
                Nothing
---------------------------------------------------------------------------------------------
'''
def persistpredictedDataPath(idx, save_file_idx = 0):
    print (f'\t\t--- Start saving the predicted Data for day: {save_file_idx} --- ')
    df_pred = pd.DataFrame({'PREDICTED': storeDaysPrediction[save_file_idx]})
    fileName = f"Error_Model_{idx}_Day_0.csv"
    df_pred.to_csv(f'{predictedDataPath}/{fileName}', index = False) 
    storeDaysPrediction[save_file_idx].clear() # Erased data so that next error model can use same index to put data temporary.
    print (f"\t\t--- Done with the Saving --- ")

'''
---------------------------------------------------------------------------------------------
    Production Code : For Deployment
    > Input : dataframe(df), inputSize(60), outputSize(30), learningRate(*), batchSize(*)
    
    > What we need to persist:
        1) 
            i)  curDay represents the index of the Date : in the statename_mandiname_Price.csv file
            ii) nextTrainDay also represent the index of the Date : in the statename_mandiname_Price.csv file, It may point to some value in future.
                In actual we will set curDay index of one less than the index of today's actual date. 
            iii) nextRecommendationDay
            iv) Saved File Index
            
        2) : In Day_{save_file_idx}.csv : The range of value variable "i" can take {0, 1, 2, ..., 29} And we need this also persistent, inorder to get the right file to save the predicted values.
                                          Make sure it follow cycles. 

    > Parameters:
        idx                   : Error model index, idx = 0 represents main index and for idx >= 1 it represents error dataset which is generated from original dataset.
        curDay                : index of the current day in the df.
        nextTrainDay          : index of the nextTrain day in the df. (at a frequency of 15 days)
        nextRecommendationDay : index of the recommendation day. Day when release the recommendation
                               for next 30 days. Output contains multiple param obtained from prospec theory
        save_file_idx         : Day_{i}, where i belongs to (0, 29). Tell the index of file to which forecast of next 30 
                                days need to append
        
        File used to persist  : data.dump (stores all the above parameter)
                              : For scalers they are x0Scaler, x1Scaler, yScaler.
                              : And model is saved in Folder Saved_Model

    Imp: 
        curDay should point to first day of (i-1)th 30 days interval if forecasting needs to be done for (i)th 30 days interval.
        Therefore, curDayIdx = forecastDayIndex - 30.
        getTrainTestSplit split the dataframe into two chunks with curDayIdx - 30 and other 30 (is never used for training).   
        prepareTrainData: last 90 days window didn't include its last 30 days in any training set and thus from currDayIdx last 60 days 
        will never be used by any of the previous model in its training set. 
    
 
---------------------------------------------------------------------------------------------                
'''
def productionCode(idx, sourceDf, targetDf, inputSize, outputSize, learningRate, batchSize):
    curDay = nextTrainDay =  5493 - 30 # targetDf.shape[0] - 4 #TODO change this at final running : Note IMP: len(df) - 60 to start it atleast one month earlier than the month for predictions.
    nextRecommendationDay = -1
    save_file_idx = 0
    # -------- Load Data from persist storage, if it exist(In general it will always) except only when this program run 1st time: 
    if (os.path.exists(f'{persistDataPath}/data_{idx}.dump')):
        print ('\t\tReading from persist storage')
        with open(f'{persistDataPath}/data_{idx}.dump', "rb") as f:
            curDay, nextTrainDay, save_file_idx, nextRecommendationDay = pickle.load(f)
    print (f'\t\tcurDay: {curDay}, nextTrainDay: {nextTrainDay}, save_file_idx: {save_file_idx}, recommend day : {nextRecommendationDay}')
    # --------- Setup the start Date (training will happen)
    sourceDx = sourceDf.iloc[:curDay]
    targetDx = targetDf.iloc[:curDay]
    predictEveryDay(idx, sourceDx, targetDx, save_file_idx, curDay, nextTrainDay, inputSize, outputSize, learningRate, batchSize)
    persistpredictedDataPath(idx, save_file_idx)
    # ---------- Store Data to the persistent storage like save_file_idx and nextTrainDay 
    if curDay == nextTrainDay:
        nextTrainDay += 15
    save_file_idx = (save_file_idx + 1) % 30
    curDay = curDay + 1
    data = [curDay, nextTrainDay, save_file_idx, nextRecommendationDay]
    with open(f'{persistDataPath}/data_{idx}.dump', "wb") as f:
        pickle.dump(data, f)


def finalModelCode():
    '''
    -------------------------------
    Final Running of code  : 
    -------------------------------
    '''
    # Setting hyper-parameters, These hyperparameter are generated after testing on several set of parameters.
    inputSize, outputSize = 60, 30
    learningRate, unit1, dropout1, unit2, dropout2, batchSize = [0.01, 32, 0.3, 32, 0.1, 64]

    '''
    Reading data from the files 
        : source Mandi refer to the mandis from which our model is trained
        : target Mandi refer to those mandis for which we are doing transfer learning on already trained models from source Mandi
    '''


    sourceInputLaggingFiles = ['/' + mandiA]
    sourceInputLaggingFiles.extend([f'/{i}_{mandiA}' for i in range(1, 5)])
    sourceInputLeadingFiles = ['/' + mandiB]
    sourceInputLeadingFiles.extend([f'/{i}_{mandiB}' for i in range(1, 5)])

    '''
    Uncomment below after running it on first time. After that this will only run when it is the 
    next train day.
    '''

    # Flag are used to identify recommendation days and recommendation day idx as an unique identifier while saving the 
    isRecommendationDay, recommendationIdx = False, 0
    if (os.path.exists(f'{persistDataPath}/data_0.dump')):
        with open(f'{persistDataPath}/data_0.dump', "rb") as f:
            curDay, nextTrainDay, save_file_idx, nextRecommendationDay = pickle.load(f)
            # if (curDay == nextTrainDay):
            #     generateErrorModelDataset(4)

    # Except file at index 0, we have error model data files.
    filesPresent = updatePredictionFiles(5)
    #
    for idx in range(5):
        sourceLagMandi, sourceLeadMandi = sourceInputLaggingFiles[idx], sourceInputLeadingFiles[idx]
        sourceLaggingMandiDF = pd.read_csv(inputFilePath + sourceLagMandi) # kota
        sourceLeadingMandiDF = pd.read_csv(inputFilePath + sourceLeadMandi) # mahidpur
        targetLaggingMandiDF = pd.read_csv(inputFilePath + "/TELANGANA_ADILABAD_Price.csv")
        # Make the timeseries multivariate, by merging the data of two mandis : Assumption is we are selecting which is lead mandi and lag mandi before hand.
        sourceDf = mergeMandisData(sourceLeadingMandiDF, sourceLaggingMandiDF, lagDays = lagDays)
        targetDf = mergeMandisData(sourceLeadingMandiDF, targetLaggingMandiDF, lagDays = lagDays) # TODO verify this ( does this left out dates )
        productionCode(idx, sourceDf, targetDf, inputSize, outputSize, learningRate, batchSize)

    # If curDay is the recommendation Day. 
    if filesPresent:
        for idx in range(5):
            mergePredictionsOfModel(idx)
        recommendationIdx = curDay
        print ('\t\t----------Today is the Recommendation Day ...\n Wait prospect theory is running...')
        todayDate, prospectDF = generateRecommendation(5, recommendationIdx)
        prospectDF['DATE'] = pd.to_datetime(prospectDF['DATE'])
        prospectDF.set_index('DATE', inplace=True)
        recommendedDate = prospectDF[['PREDICTED']].idxmax().dt.strftime('%Y-%m-%d')[0]
        fileName = f'{parDir}/Data/ForecastedData/Recommendation/recommend_{recommendationIdx}.csv' 
        prospectDF.to_csv(fileName)
        print ('\t\t------Done with Prospect----\n\t\tPushing Data to the firebase ...\n')
        # push_recommendation(recommendationIdx)

#generateErrorModelDataset(4)
finalModelCode()
