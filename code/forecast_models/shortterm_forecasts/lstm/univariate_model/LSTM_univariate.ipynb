{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fedWaQpzM1nY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I8vjcQif7S-e"
      },
      "outputs": [],
      "source": [
        "def getTrainTestData(inputSize, outputSize, df):\n",
        "        total = inputSize + outputSize\n",
        "        data_train = df.iloc[:-(total), :]['ARRIVAL']\n",
        "        data_test = df.iloc[-(total):, :]['ARRIVAL']\n",
        "        data_train = np.array(data_train)\n",
        "        data_test = np.array(data_test) \n",
        "        return data_train, data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KGbqup7X7TAv"
      },
      "outputs": [],
      "source": [
        "def prepareTrainData(inputSize, outputSize, data_train):\n",
        "    total = inputSize + outputSize\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(0, len(data_train)-total):\n",
        "        x = data_train[i:i+inputSize]\n",
        "        y = data_train[i+inputSize:i+total]\n",
        "        x_train.append(x)\n",
        "        y_train.append(y)\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    return x_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SBshk5Vd7TEG"
      },
      "outputs": [],
      "source": [
        "def prepareTestData(inputSize, outputSize, data_test):\n",
        "    total = inputSize + outputSize\n",
        "    x_test, y_test = [], []\n",
        "    for i in range(0, len(data_test)-(total-1)):\n",
        "        x = data_test[i:i+inputSize]\n",
        "        y = data_test[i+inputSize:i+total]\n",
        "        x_test.append(x)\n",
        "        y_test.append(y)\n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "    return x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "edHVNsuR7TIH"
      },
      "outputs": [],
      "source": [
        "def transformData(x_train, y_train, x_test, y_test):\n",
        "    x_scalar = MaxAbsScaler()\n",
        "    y_scalar = MaxAbsScaler()\n",
        "\n",
        "    x_train = x_scalar.fit_transform(x_train)\n",
        "    y_train = y_scalar.fit_transform(y_train)\n",
        "\n",
        "    x_test = x_scalar.transform(x_test)\n",
        "    y_test = y_scalar.transform(y_test)\n",
        "    return x_train, y_train, x_test, y_test, x_scalar, y_scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SbXTodj5_wVi"
      },
      "outputs": [],
      "source": [
        "def inverseTransformData(y_pred, y_test, y_scalar):\n",
        "    y_pred = y_scalar.inverse_transform(y_pred)\n",
        "    y_true = y_scalar.inverse_transform(y_test)\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oSJgep6v7TLl"
      },
      "outputs": [],
      "source": [
        "def makeData3D(x_train, y_train, x_test):\n",
        "    x_train = np.expand_dims(x_train, axis = 2)\n",
        "    y_train = np.expand_dims(y_train, axis = 2)\n",
        "    x_test = np.expand_dims(x_test, axis = 2)\n",
        "    return x_train, y_train, x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IeEiBT5gVjyA"
      },
      "outputs": [],
      "source": [
        "# This function keeps the initial learning rate for the first ten epochs  \n",
        "# and decreases it exponentially after that.  \n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EfVKKlzI-bJI"
      },
      "outputs": [],
      "source": [
        "def trainModel(x_train, y_train, inputSize, outputSize, unit1, learningRate, batchSize, dropout1, unit2, dropout2):\n",
        "    # print('in trainModel')\n",
        "    # print(x_train.shape, y_train.shape)\n",
        "    reg = Sequential()\n",
        "    reg.add(LSTM(units = unit1, activation = 'tanh', input_shape = (inputSize,1), return_sequences = True, dropout = dropout1))\n",
        "    reg.add(LSTM(unit2, dropout = dropout2))\n",
        "    reg.add(Dense(outputSize))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learningRate)\n",
        "    reg.compile(loss = 'mse', optimizer = opt)\n",
        "\n",
        "    early_stopping = callbacks.EarlyStopping(monitor = 'val_loss',\n",
        "                                                    patience = 5,\n",
        "                                                    mode = 'min',\n",
        "                                                  min_delta = .0001,\n",
        "                                                  restore_best_weights = True)\n",
        "    # csv_logger = callbacks.CSVLogger('drive/MyDrive/MSR Thesis Documents/LSTM Forecasting/training.log')\n",
        "    lr_scheduler = callbacks.LearningRateScheduler(scheduler)\n",
        "    # logdir = \"drive/MyDrive/MSR Thesis Documents/LSTM Forecasting/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "\n",
        "    reg.fit(x_train, y_train, epochs = 100, shuffle = False, validation_split = 0.2, callbacks = [early_stopping, lr_scheduler], verbose = 0, batch_size = batchSize)\n",
        "    #reg.fit(x_train, y_train, epochs = 100, shuffle = False, validation_split = 0.2)\n",
        "    # print('out trainModel')\n",
        "    return reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2dDKkbBP-1yA"
      },
      "outputs": [],
      "source": [
        "def testModel(x_test,reg):\n",
        "    # print('in testModel')\n",
        "    # print(x_test.shape)\n",
        "    y_pred = reg.predict(x_test)\n",
        "    # print('in testModel')\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M7b9_ne47HYU"
      },
      "outputs": [],
      "source": [
        "def forecastingMonthly(inputSize, outputSize, df, unit1, learningRate, batchSize, dropout1 , unit2, dropout2):\n",
        "    data_train, data_test = getTrainTestData(inputSize, outputSize, df)   \n",
        "    x_train, y_train = prepareTrainData(inputSize, outputSize, data_train)\n",
        "    x_test, y_test = prepareTestData(inputSize, outputSize, data_test)\n",
        "    x_train, y_train, x_test, y_test, x_scalar, y_scaclar = transformData(x_train, y_train, x_test, y_test)\n",
        "    x_train, y_train, x_test = makeData3D(x_train, y_train, x_test)\n",
        "    reg = trainModel(x_train, y_train, inputSize, outputSize, unit1, learningRate, batchSize, dropout1, unit2, dropout2)\n",
        "    y_pred = testModel(x_test,reg)\n",
        "    y_true, y_pred = inverseTransformData(y_pred, y_test, y_scaclar)\n",
        "    y_true = y_true[0, :]\n",
        "    y_pred = y_pred[0, :]\n",
        "\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_gas2WOCnPpd"
      },
      "outputs": [],
      "source": [
        "def forecast(df, inputSize, unit1, learningRate, batchSize, dropout1, unit2, dropout2):\n",
        "    start = df[df['DATE']=='2020-06-01'].index[0]     # read the index of the data 2020-06-01\n",
        "    end = len(df)                                     # end is the length of data frame.\n",
        "\n",
        "    predicted, actual = [], []                        \n",
        "    final = pd.DataFrame()                            \n",
        "    map = {}\n",
        "    while(start<end):\n",
        "        if(start+30<end):\n",
        "            outputSize = 30\n",
        "            start = start + 1\n",
        "            dx = df.iloc[:start]\n",
        "            y_true, y_pred = forecastingMonthly(inputSize, outputSize, dx, unit1, learningRate, batchSize, dropout1, unit2, dropout2)\n",
        "            for i in range(outputSize):\n",
        "              map[df.iloc[start + i]['DATE']] += [y_pred[i]]\n",
        "            #actual.extend(y_true)\n",
        "            #predicted.extend(y_pred)\n",
        "        else:\n",
        "            outputSize = end - start\n",
        "            dx = df\n",
        "            y_true, y_pred = forecastingMonthly(inputSize, outputSize, dx, unit1, learningRate, batchSize, dropout1, unit2, dropout2)\n",
        "            for i in range(outputSize):\n",
        "              map[df.iloc[start + i]['DATE']] += [y_pred[i]]\n",
        "            actual.extend(y_true)\n",
        "            predicted.extend(y_pred)\n",
        "            start = start + 1\n",
        "            #break\n",
        "    date, value = [], []\n",
        "    for i in map:\n",
        "      date.append(i)\n",
        "      value.append(map[i])\n",
        "\n",
        "    final = pd.DataFrame({'DATE' : date, 'PREDICTED' : value})\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AVpbkWpvqon_"
      },
      "outputs": [],
      "source": [
        "def rmse30DayWindow(df):\n",
        "    mse = (df['ACTUAL'] - df[\"PREDICTED\"])**2\n",
        "    rmse = (mse.mean())**.5\n",
        "    return rmse\n",
        "\n",
        "def rmse1DayWindow(df):\n",
        "    mse = (df['ACTUAL'] - df[\"PREDICTED\"])**2\n",
        "    rmse = (mse.mean())**.5\n",
        "    return rmse/len(df)    \n",
        "\n",
        "\n",
        "def normalizedRmse1Day(df):\n",
        "    mse = (df['ACTUAL'] - df[\"PREDICTED\"])**2\n",
        "    rmse = ((mse.sum())**.5)/df['ACTUAL'].mean()\n",
        "    return rmse/len(df) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qcTCLlZaXup0"
      },
      "outputs": [],
      "source": [
        "def rmse(df):\n",
        "    df.columns = ['ACTUAL', 'PREDICTED']\n",
        "    l30, l1, lnormalized = [], [], []\n",
        "    for i in range(0,len(df),30):\n",
        "        x30 = rmse30DayWindow(df[i:i+30])\n",
        "        x1 = rmse1DayWindow(df[i:i+30])\n",
        "        xnorm = normalizedRmse1Day(df[i:i+30])\n",
        "        l30.append(x30)\n",
        "        l1.append(x1)\n",
        "        lnormalized.append(xnorm)\n",
        "    return l30, l1, lnormalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JxYRqjygVMJ",
        "outputId": "a42d6008-f51f-438f-90c5-764735b14f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 4 2 4 2 4\n",
            "[[0.001, 32, 0.1, 32, 0.2, 128], [0.001, 64, 0.1, 16, 0.2, 128], [0.001, 16, 0.2, 64, 0.1, 512], [0.001, 64, 0.2, 16, 0.2, 512], [0.001, 32, 0.1, 64, 0.2, 256], [0.001, 16, 0.1, 32, 0.2, 128], [0.001, 128, 0.1, 64, 0.1, 256], [0.001, 128, 0.1, 64, 0.1, 64], [0.001, 16, 0.2, 16, 0.2, 256], [0.001, 16, 0.2, 64, 0.2, 64]]\n",
            "[0.001, 32, 0.1, 32, 0.2, 128]\n",
            "name_of_file_0.001_32_0.1_32_0.2_128.csv\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'2020-06-02'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21328/1713537299.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# asking forecast to performa things\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileToSave\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# lstmDf = pd.read_csv('drive/MyDrive/MSR Thesis Documents/LSTM Forecasting/'+fileToSave)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21328/2542229256.py\u001b[0m in \u001b[0;36mforecast\u001b[1;34m(df, inputSize, unit1, learningRate, batchSize, dropout1, unit2, dropout2)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforecastingMonthly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m               \u001b[0mmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;31m#actual.extend(y_true)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m#predicted.extend(y_pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: '2020-06-02'"
          ]
        }
      ],
      "source": [
        "random.seed(20)\n",
        "learningRates = [.001]\n",
        "units1 = [16, 32, 64, 128]\n",
        "dropouts1 = [0.1, 0.2]\n",
        "units2 = [16, 32, 64, 128]\n",
        "dropouts2 = [0.1, 0.2]\n",
        "batchSizes = [64, 128, 256, 512]\n",
        "\n",
        "print(len(learningRates), len(units1), len(dropouts1), len(units2), len(dropouts2), len(batchSizes))\n",
        "\n",
        "# trying 10 random samples : \n",
        "zipped = random.sample([[i,j,k, l, m, n] for i in learningRates for j in units1 for k in dropouts1 for l in units2 for m in dropouts2 for n in batchSizes], 10)\n",
        "\n",
        "print(zipped)\n",
        "for com in zipped:\n",
        "    print(com)\n",
        "    # for each sample set the parameter: \n",
        "    learningRate = com[0]\n",
        "    unit1 = com[1]\n",
        "    dropout1 = com[2]\n",
        "    unit2 = com[3]\n",
        "    dropout2 = com[4]\n",
        "    batchSize = com[5]\n",
        "    # ask panda to read_csv:\n",
        "    dir = 'RAJASTHAN_KOTA_Arrival.csv'\n",
        "    df = pd.read_csv(dir)\n",
        "    name_of_file = dir.replace('.csv', '')\n",
        "    # file to save with specific name: \n",
        "    fileToSave = \"name_of_file\"+\"_{}_{}_{}_{}_{}_{}.csv\".format(com[0], com[1], com[2], com[3], com[4], com[5])\n",
        "    print(fileToSave)\n",
        "    \n",
        "    # asking forecast to performa things\n",
        "    final = forecast(df, 60, unit1, learningRate, batchSize, dropout1, unit2, dropout2)\n",
        "    final.to_csv(fileToSave, index = False)\n",
        "    # lstmDf = pd.read_csv('drive/MyDrive/MSR Thesis Documents/LSTM Forecasting/'+fileToSave)\n",
        "    # l30, l1, lnormalized = rmse(lstmDf)\n",
        "    # print(np.mean(l30), np.mean(l1), np.mean(lnormalized))\n",
        "    print('-'*20)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3aNMDmbkrsR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lstm univariate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
